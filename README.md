# Toxicity-Detection-BERT
# ðŸ§ª Toxicity Detection using BERT

This project uses a fine-tuned BERT model to perform **multi-label classification** on toxic comments. It is based on the [Jigsaw Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge) dataset. The goal is to detect various types of toxicity such as **toxic**, **severe toxic**, **obscene**, **threat**, **insult**, and **identity hate**.

---

## ðŸ“Œ Project Highlights

- âœ… Multi-label classification with six toxicity labels  
- âœ… Fine-tuned `bert-base-uncased` model using Hugging Face Transformers  
- âœ… Tokenization, model building, training, and evaluation all done in Python (Jupyter Notebook)  
- âœ… Evaluation with **accuracy**, **macro F1-score**, and **ROC-AUC per label**

---



